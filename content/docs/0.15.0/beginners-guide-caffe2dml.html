<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <title>Beginner's Guide for Caffe2DML users - SystemML 0.15.0</title>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        
        <meta name="description" content="Beginner's Guide for Caffe2DML users">
        
        <meta name="viewport" content="width=device-width">
        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/main.css">
        <link rel="stylesheet" href="css/pygments-default.css">
        <link rel="shortcut icon" href="img/favicon.png">
    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an outdated browser. <a href="http://browsehappy.com/">Upgrade your browser today</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to better experience this site.</p>
        <![endif]-->

        <header class="navbar navbar-default navbar-fixed-top" id="topbar">
            <div class="container">
                <div class="navbar-header">
                    <div class="navbar-brand brand projectlogo">
                        <a href="http://systemml.apache.org/"><img class="logo" src="img/systemml-logo.png" alt="Apache SystemML" title="Apache SystemML"/></a>
                    </div>
                    <div class="navbar-brand brand projecttitle">
                        <a href="http://systemml.apache.org/">Apache SystemML<sup id="trademark">â„¢</sup></a><br/>
                        <span class="version">0.15.0</span>
                    </div>
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                </div>
                <nav class="navbar-collapse collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li><a href="index.html">Overview</a></li>
                        <li><a href="https://github.com/apache/systemml">GitHub</a></li>
                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Documentation<b class="caret"></b></a>
                            <ul class="dropdown-menu" role="menu">
                                <li><b>Running SystemML:</b></li>
                                <li><a href="https://github.com/apache/systemml">SystemML GitHub README</a></li>
                                <li><a href="spark-mlcontext-programming-guide.html">Spark MLContext</a></li>
                                <li><a href="spark-batch-mode.html">Spark Batch Mode</a>
                                <li><a href="hadoop-batch-mode.html">Hadoop Batch Mode</a>
                                <li><a href="standalone-guide.html">Standalone Guide</a></li>
                                <li><a href="jmlc.html">Java Machine Learning Connector (JMLC)</a>
                                <li class="divider"></li>
                                <li><b>Language Guides:</b></li>
                                <li><a href="dml-language-reference.html">DML Language Reference</a></li>
                                <li><a href="beginners-guide-to-dml-and-pydml.html">Beginner's Guide to DML and PyDML</a></li>
                                <li><a href="beginners-guide-python.html">Beginner's Guide for Python Users</a></li>
                                <li><a href="python-reference.html">Reference Guide for Python Users</a></li>
                                <li class="divider"></li>
                                <li><b>ML Algorithms:</b></li>
                                <li><a href="algorithms-reference.html">Algorithms Reference</a></li>
                                <li class="divider"></li>
                                <li><b>Tools:</b></li>
                                <li><a href="debugger-guide.html">Debugger Guide</a></li>
                                <li><a href="developer-tools-systemml.html">IDE Guide</a></li>
                                <li class="divider"></li>
                                <li><b>Other:</b></li>
                                <li><a href="contributing-to-systemml.html">Contributing to SystemML</a></li>
                                <li><a href="engine-dev-guide.html">Engine Developer Guide</a></li>
                                <li><a href="troubleshooting-guide.html">Troubleshooting Guide</a></li>
                                <li><a href="release-process.html">Release Process</a></li>
                            </ul>
                        </li>
                        
                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">API Docs<b class="caret"></b></a>
                            <ul class="dropdown-menu" role="menu">
                                <li><a href="./api/java/index.html">Java</a></li>
                                <li><a href="./api/python/index.html">Python</a></li>
                            </ul>
                        </li>
                        
                        <li class="dropdown">
                            <a href="#" class="dropdown-toggle" data-toggle="dropdown">Issues<b class="caret"></b></a>
                            <ul class="dropdown-menu" role="menu">
                                <li><b>JIRA:</b></li>
                                <li><a href="https://issues.apache.org/jira/browse/SYSTEMML">SystemML JIRA</a></li>
                                
                            </ul>
                        </li>
                    </ul>
                </nav>
            </div>
        </header>

        <div class="container" id="content">
          
            <h1 class="title">Beginner's Guide for Caffe2DML users</h1>
          

          <!--

-->

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="#training-lenet" id="markdown-toc-training-lenet">Training Lenet</a></li>
      <li><a href="#additional-configuration" id="markdown-toc-additional-configuration">Additional Configuration</a></li>
      <li><a href="#saving-the-trained-model" id="markdown-toc-saving-the-trained-model">Saving the trained model</a></li>
      <li><a href="#loading-a-pretrained-caffemodel" id="markdown-toc-loading-a-pretrained-caffemodel">Loading a pretrained caffemodel</a></li>
    </ul>
  </li>
  <li><a href="#frequently-asked-questions" id="markdown-toc-frequently-asked-questions">Frequently asked questions</a>    <ul>
      <li><a href="#what-is-the-purpose-of-caffe2dml-api-" id="markdown-toc-what-is-the-purpose-of-caffe2dml-api-">What is the purpose of Caffe2DML API ?</a></li>
      <li><a href="#with-caffe2dml-does-systemml-now-require-caffe-to-be-installed-" id="markdown-toc-with-caffe2dml-does-systemml-now-require-caffe-to-be-installed-">With Caffe2DML, does SystemML now require Caffe to be installed ?</a></li>
      <li><a href="#how-can-i-speedup-the-training-with-caffe2dml-" id="markdown-toc-how-can-i-speedup-the-training-with-caffe2dml-">How can I speedup the training with Caffe2DML ?</a></li>
      <li><a href="#how-to-enable-gpu-support-in-caffe2dml-" id="markdown-toc-how-to-enable-gpu-support-in-caffe2dml-">How to enable GPU support in Caffe2DML ?</a></li>
      <li><a href="#what-is-lrpolicy-in-the-solver-specification-" id="markdown-toc-what-is-lrpolicy-in-the-solver-specification-">What is lr_policy in the solver specification ?</a></li>
      <li><a href="#how-to-set-batch-size-" id="markdown-toc-how-to-set-batch-size-">How to set batch size ?</a></li>
      <li><a href="#how-to-set-maximum-number-of-iterations-for-training-" id="markdown-toc-how-to-set-maximum-number-of-iterations-for-training-">How to set maximum number of iterations for training ?</a></li>
      <li><a href="#how-to-set-the-size-of-the-validation-dataset-" id="markdown-toc-how-to-set-the-size-of-the-validation-dataset-">How to set the size of the validation dataset ?</a></li>
      <li><a href="#how-to-monitor-loss-via-command-line-" id="markdown-toc-how-to-monitor-loss-via-command-line-">How to monitor loss via command-line ?</a></li>
      <li><a href="#how-to-pass-a-single-jpeg-image-to-caffe2dml-for-prediction-" id="markdown-toc-how-to-pass-a-single-jpeg-image-to-caffe2dml-for-prediction-">How to pass a single jpeg image to Caffe2DML for prediction ?</a></li>
      <li><a href="#how-to-prepare-a-directory-of-jpeg-images-for-training-with-caffe2dml-" id="markdown-toc-how-to-prepare-a-directory-of-jpeg-images-for-training-with-caffe2dml-">How to prepare a directory of jpeg images for training with Caffe2DML ?</a></li>
      <li><a href="#can-i-use-caffe2dml-via-scala-" id="markdown-toc-can-i-use-caffe2dml-via-scala-">Can I use Caffe2DML via Scala ?</a></li>
      <li><a href="#how-can-i-get-summary-information-of-my-network-" id="markdown-toc-how-can-i-get-summary-information-of-my-network-">How can I get summary information of my network ?</a></li>
      <li><a href="#how-can-i-view-the-script-generated-by-caffe2dml-" id="markdown-toc-how-can-i-view-the-script-generated-by-caffe2dml-">How can I view the script generated by Caffe2DML ?</a></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="introduction">Introduction</h2>

<p>Caffe2DML is an <strong>experimental API</strong> that converts an Caffe specification to DML. 
It is designed to fit well into the mllearn framework and hence supports NumPy, Pandas as well as PySpark DataFrame.</p>

<h3 id="training-lenet">Training Lenet</h3>

<p>To create a Caffe2DML object, one needs to create a solver and network file that conforms 
to the <a href="http://caffe.berkeleyvision.org/">Caffe specification</a>.
In this example, we will train Lenet which is a simple convolutional neural network, proposed by Yann LeCun in 1998. 
It has 2 convolutions/pooling and fully connected layer. 
Similar to Caffe, the network has been modified to add dropout. 
For more detail, please see <a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a>.</p>

<p>The <a href="https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet_solver.proto">solver specification</a>
specifies to Caffe2DML to use following configuration when generating the training DML script:</p>
<ul>
  <li><code class="highlighter-rouge">type: "SGD", momentum: 0.9</code>: Stochastic Gradient Descent with momentum optimizer with <code class="highlighter-rouge">momentum=0.9</code>.</li>
  <li><code class="highlighter-rouge">lr_policy: "exp", gamma: 0.95, base_lr: 0.01</code>: Use exponential decay learning rate policy (<code class="highlighter-rouge">base_lr * gamma ^ iter</code>).</li>
  <li><code class="highlighter-rouge">display: 100</code>: Display training loss after every 100 iterations.</li>
  <li><code class="highlighter-rouge">test_interval: 500</code>: Display validation loss after every 500 iterations.</li>
  <li><code class="highlighter-rouge">test_iter: 10</code>: Validation data size = 10 * BATCH_SIZE.</li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">systemml.mllearn</span> <span class="kn">import</span> <span class="n">Caffe2DML</span>
<span class="kn">import</span> <span class="nn">urllib</span>

<span class="c"># Download the Lenet network</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet.proto'</span><span class="p">,</span> <span class="s">'lenet.proto'</span><span class="p">)</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/mnist_lenet/lenet_solver.proto'</span><span class="p">,</span> <span class="s">'lenet_solver.proto'</span><span class="p">)</span>
<span class="c"># Train Lenet On MNIST using scikit-learn like API</span>

<span class="c"># MNIST dataset contains 28 X 28 gray-scale (number of channel=1).</span>
<span class="n">lenet</span> <span class="o">=</span> <span class="n">Caffe2DML</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'lenet_solver.proto'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">lenet</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre>
</div>

<p>Output:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>+-----+---------------+--------------+------------+---------+-----------+---------+
| Name|           Type|        Output|      Weight|     Bias|        Top|   Bottom|
+-----+---------------+--------------+------------+---------+-----------+---------+
|mnist|           Data| (, 1, 28, 28)|            |         |mnist,mnist|         |
|conv1|    Convolution|(, 32, 28, 28)|   [32 X 25]| [32 X 1]|      conv1|    mnist|
|relu1|           ReLU|(, 32, 28, 28)|            |         |      relu1|    conv1|
|pool1|        Pooling|(, 32, 14, 14)|            |         |      pool1|    relu1|
|conv2|    Convolution|(, 64, 14, 14)|  [64 X 800]| [64 X 1]|      conv2|    pool1|
|relu2|           ReLU|(, 64, 14, 14)|            |         |      relu2|    conv2|
|pool2|        Pooling|  (, 64, 7, 7)|            |         |      pool2|    relu2|
|  ip1|   InnerProduct| (, 512, 1, 1)|[3136 X 512]|[1 X 512]|        ip1|    pool2|
|relu3|           ReLU| (, 512, 1, 1)|            |         |      relu3|      ip1|
|drop1|        Dropout| (, 512, 1, 1)|            |         |      drop1|    relu3|
|  ip2|   InnerProduct|  (, 10, 1, 1)|  [512 X 10]| [1 X 10]|        ip2|    drop1|
| loss|SoftmaxWithLoss|  (, 10, 1, 1)|            |         |       loss|ip2,mnist|
+-----+---------------+--------------+------------+---------+-----------+---------+
</code></pre>
</div>

<p>To train the above lenet model, we use the MNIST dataset. 
The MNIST dataset was constructed from two datasets of the US National Institute of Standards and Technology (NIST). 
The training set consists of handwritten digits from 250 different people, 50 percent high school students, and 50 percent employees from the Census Bureau. Note that the test set contains handwritten digits from different people following the same split.
In this example, we are using mlxtend package to load the mnist dataset into Python NumPy arrays, but you are free to download it directly from http://yann.lecun.com/exdb/mnist/.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>pip install mlxtend
</code></pre>
</div>

<p>We first split the MNIST dataset into train and test.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mlxtend.data</span> <span class="kn">import</span> <span class="n">mnist_data</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="c"># Download the MNIST dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mnist_data</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c"># Split the data into training and test</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="o">.</span><span class="mi">9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="o">.</span><span class="mi">9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="o">.</span><span class="mi">9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">):]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="o">.</span><span class="mi">9</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">):]</span>
</code></pre>
</div>

<p>Finally, we use the training and test dataset to perform training and prediction using scikit-learn like API.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Since Caffe2DML is a mllearn API, it allows for scikit-learn like method for training.</span>
<span class="n">lenet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c"># Either perform prediction: lenet.predict(X_test) or scoring:</span>
<span class="n">lenet</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre>
</div>

<p>Output:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>Iter:100, training loss:0.189008481420049, training accuracy:92.1875
Iter:200, training loss:0.21657020576713149, training accuracy:96.875
Iter:300, training loss:0.05780939180052287, training accuracy:98.4375
Iter:400, training loss:0.03406193840071965, training accuracy:100.0
Iter:500, training loss:0.02847187709112875, training accuracy:100.0
Iter:500, validation loss:222.736109642486, validation accuracy:96.49077868852459
Iter:600, training loss:0.04867848427394318, training accuracy:96.875
Iter:700, training loss:0.043060905384304224, training accuracy:98.4375
Iter:800, training loss:0.01861298388336358, training accuracy:100.0
Iter:900, training loss:0.03495462005933769, training accuracy:100.0
Iter:1000, training loss:0.04598737325942163, training accuracy:98.4375
Iter:1000, validation loss:180.04232316810746, validation accuracy:97.28483606557377
Iter:1100, training loss:0.05630274512793694, training accuracy:98.4375
Iter:1200, training loss:0.027278141291535066, training accuracy:98.4375
Iter:1300, training loss:0.04356275106270366, training accuracy:98.4375
Iter:1400, training loss:0.00780793048139091, training accuracy:100.0
Iter:1500, training loss:0.004135965492374173, training accuracy:100.0
Iter:1500, validation loss:156.61636761709374, validation accuracy:97.48975409836065
Iter:1600, training loss:0.007939063305475983, training accuracy:100.0
Iter:1700, training loss:0.0025769653351162196, training accuracy:100.0
Iter:1800, training loss:0.0023251742357435204, training accuracy:100.0
Iter:1900, training loss:0.0016795711023936644, training accuracy:100.0
Iter:2000, training loss:0.03676045262879483, training accuracy:98.4375
Iter:2000, validation loss:173.66147359346, validation accuracy:97.48975409836065
0.97399999999999998
</code></pre>
</div>

<h3 id="additional-configuration">Additional Configuration</h3>

<ul>
  <li>Print the generated DML script along with classification report:  <code class="highlighter-rouge">lenet.set(debug=True)</code></li>
  <li>Print the heavy hitters instruction and the execution plan (advanced users): <code class="highlighter-rouge">lenet.setStatistics(True).setExplain(True)</code></li>
  <li>(Optional but recommended) Enable <a href="http://apache.github.io/systemml/native-backend">native BLAS</a>: <code class="highlighter-rouge">lenet.setConfigProperty("native.blas", "auto")</code></li>
  <li>Enable experimental feature such as codegen: <code class="highlighter-rouge">lenet.setConfigProperty("codegen.enabled", "true").setConfigProperty("codegen.plancache", "true")</code></li>
  <li>Force GPU execution (please make sure the required jcuda dependency are included): lenet.setGPU(True).setForceGPU(True)</li>
</ul>

<p>Unlike Caffe where default train and test algorithm is <code class="highlighter-rouge">minibatch</code>, you can specify the
algorithm using the parameters <code class="highlighter-rouge">train_algo</code> and <code class="highlighter-rouge">test_algo</code> (valid values are: <code class="highlighter-rouge">minibatch</code>, <code class="highlighter-rouge">allreduce_parallel_batches</code>, 
and <code class="highlighter-rouge">allreduce</code>). Here are some common settings:</p>

<table>
  <thead>
    <tr>
      <th>&#160;</th>
      <th>PySpark script</th>
      <th>Changes to Network/Solver</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Single-node CPU execution (similar to Caffe with solver_mode: CPU)</td>
      <td><code class="highlighter-rouge">lenet.set(train_algo="minibatch", test_algo="minibatch")</code></td>
      <td>Ensure that <code class="highlighter-rouge">batch_size</code> is set to appropriate value (for example: 64)</td>
    </tr>
    <tr>
      <td>Single-node single-GPU execution</td>
      <td><code class="highlighter-rouge">lenet.set(train_algo="minibatch", test_algo="minibatch").setGPU(True).setForceGPU(True)</code></td>
      <td>Ensure that <code class="highlighter-rouge">batch_size</code> is set to appropriate value (for example: 64)</td>
    </tr>
    <tr>
      <td>Single-node multi-GPU execution (similar to Caffe with solver_mode: GPU)</td>
      <td><code class="highlighter-rouge">lenet.set(train_algo="allreduce_parallel_batches", test_algo="minibatch", parallel_batches=num_gpu).setGPU(True).setForceGPU(True)</code></td>
      <td>Ensure that <code class="highlighter-rouge">batch_size</code> is set to appropriate value (for example: 64)</td>
    </tr>
    <tr>
      <td>Distributed prediction</td>
      <td><code class="highlighter-rouge">lenet.set(test_algo="allreduce")</code></td>
      <td>&#160;</td>
    </tr>
    <tr>
      <td>Distributed synchronous training</td>
      <td><code class="highlighter-rouge">lenet.set(train_algo="allreduce_parallel_batches", parallel_batches=num_cluster_cores)</code></td>
      <td>Ensure that <code class="highlighter-rouge">batch_size</code> is set to appropriate value (for example: 64)</td>
    </tr>
  </tbody>
</table>

<h3 id="saving-the-trained-model">Saving the trained model</h3>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">lenet</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lenet</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s">'trained_weights'</span><span class="p">)</span>
<span class="n">new_lenet</span> <span class="o">=</span> <span class="n">Caffe2DML</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'lenet_solver.proto'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
<span class="n">new_lenet</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'trained_weights'</span><span class="p">)</span>
<span class="n">new_lenet</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</code></pre>
</div>

<h3 id="loading-a-pretrained-caffemodel">Loading a pretrained caffemodel</h3>

<p>We provide a converter utility to convert <code class="highlighter-rouge">.caffemodel</code> trained using Caffe to SystemML format.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># First download deploy file and caffemodel</span>
<span class="kn">import</span> <span class="nn">urllib</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/imagenet/vgg19/VGG_ILSVRC_19_layers_deploy.proto'</span><span class="p">,</span> <span class="s">'VGG_ILSVRC_19_layers_deploy.proto'</span><span class="p">)</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel'</span><span class="p">,</span> <span class="s">'VGG_ILSVRC_19_layers.caffemodel'</span><span class="p">)</span>
<span class="c"># Save the weights into trained_vgg_weights directory</span>
<span class="kn">import</span> <span class="nn">systemml</span> <span class="kn">as</span> <span class="nn">sml</span>
<span class="n">sml</span><span class="o">.</span><span class="n">convert_caffemodel</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="s">'VGG_ILSVRC_19_layers_deploy.proto'</span><span class="p">,</span> <span class="s">'VGG_ILSVRC_19_layers.caffemodel'</span><span class="p">,</span>  <span class="s">'trained_vgg_weights'</span><span class="p">)</span>
</code></pre>
</div>

<p>We can then use the <code class="highlighter-rouge">trained_vgg_weights</code> directory for performing prediction or fine-tuning.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Download the VGG network</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/imagenet/vgg19/VGG_ILSVRC_19_layers_network.proto'</span><span class="p">,</span> <span class="s">'VGG_ILSVRC_19_layers_network.proto'</span><span class="p">)</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/imagenet/vgg19/VGG_ILSVRC_19_layers_solver.proto'</span><span class="p">,</span> <span class="s">'VGG_ILSVRC_19_layers_solver.proto'</span><span class="p">)</span>
<span class="c"># Storing the labels.txt in the weights directory allows predict to return a label (for example: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor') rather than the column index of one-hot encoded vector (for example: 287).</span>
<span class="n">urllib</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="s">'https://raw.githubusercontent.com/apache/systemml/master/scripts/nn/examples/caffe2dml/models/imagenet/labels.txt'</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'trained_vgg_weights'</span><span class="p">,</span> <span class="s">'labels.txt'</span><span class="p">))</span>
<span class="kn">from</span> <span class="nn">systemml.mllearn</span> <span class="kn">import</span> <span class="n">Caffe2DML</span>
<span class="n">vgg</span> <span class="o">=</span> <span class="n">Caffe2DML</span><span class="p">(</span><span class="n">sqlCtx</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'VGG_ILSVRC_19_layers_solver.proto'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="n">vgg</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'trained_vgg_weights'</span><span class="p">)</span>
<span class="c"># We can then perform prediction:</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">sml</span><span class="o">.</span><span class="n">convertImageToNumPyArr</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'test.jpg'</span><span class="p">),</span> <span class="n">img_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="n">vgg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c"># OR Fine-Tuning: vgg.fit(X_train, y_train)</span>
</code></pre>
</div>

<h2 id="frequently-asked-questions">Frequently asked questions</h2>

<h4 id="what-is-the-purpose-of-caffe2dml-api-">What is the purpose of Caffe2DML API ?</h4>

<p>Most deep learning experts are more likely to be familiar with the Caffe&#8217;s specification
rather than DML language. For these users, the Caffe2DML API reduces the learning curve to using SystemML.
Instead of requiring the users to write a DML script for training, fine-tuning and testing the model,
Caffe2DML takes as an input a network and solver specified in the Caffe specification
and automatically generates the corresponding DML.</p>

<h4 id="with-caffe2dml-does-systemml-now-require-caffe-to-be-installed-">With Caffe2DML, does SystemML now require Caffe to be installed ?</h4>

<p>Absolutely not. We only support Caffe&#8217;s API for convenience of the user as stated above.
Since the Caffe&#8217;s API is specified in the protobuf format, we are able to generate the java parser files
and donot require Caffe to be installed. This is also true for Tensorboard feature of Caffe2DML.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Dml.g4      ---&gt; antlr  ---&gt; DmlLexer.java, DmlListener.java, DmlParser.java ---&gt; parse foo.dml
caffe.proto ---&gt; protoc ---&gt; target/generated-sources/caffe/Caffe.java       ---&gt; parse caffe_network.proto, caffe_solver.proto 
</code></pre>
</div>

<p>Again, the SystemML engine doesnot invoke (or depend on) Caffe and TensorFlow for any of its runtime operators.
Since the grammar files for the respective APIs (i.e. <code class="highlighter-rouge">caffe.proto</code>) are used by SystemML, 
we include their licenses in our jar files.</p>

<h4 id="how-can-i-speedup-the-training-with-caffe2dml-">How can I speedup the training with Caffe2DML ?</h4>

<ul>
  <li>Enable native BLAS to improve the performance of CP convolution and matrix multiplication operators.
If you are using OpenBLAS, please ensure that it was built with <code class="highlighter-rouge">USE_OPENMP</code> flag turned on.
For more detail see http://apache.github.io/systemml/native-backend</li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">caffe2dmlObject</span><span class="o">.</span><span class="n">setConfigProperty</span><span class="p">(</span><span class="s">"native.blas"</span><span class="p">,</span> <span class="s">"auto"</span><span class="p">)</span>
</code></pre>
</div>

<ul>
  <li>Turn on the experimental codegen feature. This should help reduce unnecessary allocation cost after every binary operation.</li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">caffe2dmlObject</span><span class="o">.</span><span class="n">setConfigProperty</span><span class="p">(</span><span class="s">"codegen.enabled"</span><span class="p">,</span> <span class="s">"true"</span><span class="p">)</span><span class="o">.</span><span class="n">setConfigProperty</span><span class="p">(</span><span class="s">"codegen.plancache"</span><span class="p">,</span> <span class="s">"true"</span><span class="p">)</span>
</code></pre>
</div>

<ul>
  <li>
    <p>Tuned the <a href="http://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning">Garbage Collector</a>.</p>
  </li>
  <li>
    <p>Enable GPU support (described below).</p>
  </li>
</ul>

<h4 id="how-to-enable-gpu-support-in-caffe2dml-">How to enable GPU support in Caffe2DML ?</h4>

<p>To be consistent with other mllearn algorithms, we recommend that you use following method instead of setting 
the <code class="highlighter-rouge">solver_mode</code> in solver file.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># The below method tells SystemML optimizer to use a GPU-enabled instruction if the operands fit in the GPU memory </span>
<span class="n">caffe2dmlObject</span><span class="o">.</span><span class="n">setGPU</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="c"># The below method tells SystemML optimizer to always use a GPU-enabled instruction irrespective of the memory requirement</span>
<span class="n">caffe2dmlObject</span><span class="o">.</span><span class="n">setForceGPU</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="what-is-lrpolicy-in-the-solver-specification-">What is lr_policy in the solver specification ?</h4>

<p>The parameter <code class="highlighter-rouge">lr_policy</code> specifies the learning rate decay policy. Caffe2DML supports following policies:</p>
<ul>
  <li><code class="highlighter-rouge">fixed</code>: always return <code class="highlighter-rouge">base_lr</code>.</li>
  <li><code class="highlighter-rouge">step</code>: return <code class="highlighter-rouge">base_lr * gamma ^ (floor(iter / step))</code></li>
  <li><code class="highlighter-rouge">exp</code>: return <code class="highlighter-rouge">base_lr * gamma ^ iter</code></li>
  <li><code class="highlighter-rouge">inv</code>: return <code class="highlighter-rouge">base_lr * (1 + gamma * iter) ^ (- power)</code></li>
  <li><code class="highlighter-rouge">poly</code>: the effective learning rate follows a polynomial decay, to be zero by the max_iter. return <code class="highlighter-rouge">base_lr (1 - iter/max_iter) ^ (power)</code></li>
  <li><code class="highlighter-rouge">sigmoid</code>: the effective learning rate follows a sigmod decay return b<code class="highlighter-rouge">ase_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</code></li>
</ul>

<h4 id="how-to-set-batch-size-">How to set batch size ?</h4>

<p>Batch size is set in <code class="highlighter-rouge">data_param</code> of the Data layer:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  data_param {
    source: "mnist_train"
    batch_size: 64
    backend: LMDB
  }
}
</code></pre>
</div>

<h4 id="how-to-set-maximum-number-of-iterations-for-training-">How to set maximum number of iterations for training ?</h4>

<p>The maximum number of iterations can be set in the solver specification</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># The maximum number of iterations</span>
max_iter: 2000
</code></pre>
</div>

<h4 id="how-to-set-the-size-of-the-validation-dataset-">How to set the size of the validation dataset ?</h4>

<p>The size of the validation dataset is determined by the parameters <code class="highlighter-rouge">test_iter</code> and the batch size. For example: If the batch size is 64 and 
<code class="highlighter-rouge">test_iter</code> is 10, then the validation size is 640. This setting generates following DML code internally:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">num_images</span> <span class="o">=</span> <span class="n">nrow</span><span class="p">(</span><span class="n">y_full</span><span class="p">)</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">num_validation</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X_full</span><span class="p">[(</span><span class="n">num_validation</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="n">num_images</span><span class="p">,];</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_full</span><span class="p">[(</span><span class="n">num_validation</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="n">num_images</span><span class="p">,]</span>
<span class="n">X_val</span> <span class="o">=</span> <span class="n">X_full</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_validation</span><span class="p">,];</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">y_full</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">num_validation</span><span class="p">,]</span>
<span class="n">num_images</span> <span class="o">=</span> <span class="n">nrow</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="how-to-monitor-loss-via-command-line-">How to monitor loss via command-line ?</h4>

<p>To monitor loss, please set following parameters in the solver specification</p>

<div class="highlighter-rouge"><pre class="highlight"><code># Display training loss and accuracy every 100 iterations
display: 100
# Carry out validation every 500 training iterations and display validation loss and accuracy.
test_iter: 10
test_interval: 500
</code></pre>
</div>

<h4 id="how-to-pass-a-single-jpeg-image-to-caffe2dml-for-prediction-">How to pass a single jpeg image to Caffe2DML for prediction ?</h4>

<p>To convert a jpeg into NumPy matrix, you can use the <a href="https://pillow.readthedocs.io/">pillow package</a> and 
SystemML&#8217;s  <code class="highlighter-rouge">convertImageToNumPyArr</code> utility function. The below pyspark code demonstrates the usage:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">systemml</span> <span class="kn">as</span> <span class="nn">sml</span>
<span class="kn">from</span> <span class="nn">systemml.mllearn</span> <span class="kn">import</span> <span class="n">Caffe2DML</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">input_image</span> <span class="o">=</span> <span class="n">sml</span><span class="o">.</span><span class="n">convertImageToNumPyArr</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">img_file_path</span><span class="p">),</span> <span class="n">img_shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
<span class="n">resnet</span> <span class="o">=</span> <span class="n">Caffe2DML</span><span class="p">(</span><span class="n">sqlCtx</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'ResNet_50_solver.proto'</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s">'ResNet_50_pretrained_weights'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
<span class="n">resnet</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_image</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="how-to-prepare-a-directory-of-jpeg-images-for-training-with-caffe2dml-">How to prepare a directory of jpeg images for training with Caffe2DML ?</h4>

<p>The below pyspark code assumes that the input dataset has 2 labels <code class="highlighter-rouge">cat</code> and <code class="highlighter-rouge">dogs</code> and the filename has these labels as prefix.
We iterate through the directory and convert each jpeg image into pyspark.ml.linalg.Vector using pyspark.
These vectors are stored as DataFrame and randomized using Spark SQL&#8217;s <code class="highlighter-rouge">orderBy(rand())</code> function.
The DataFrame is then saved in parquet format to reduce the cost of preprocessing for repeated training.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">systemml.mllearn</span> <span class="kn">import</span> <span class="n">Caffe2DML</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">urllib</span><span class="o">,</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">scipy.ndimage</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.linalg</span> <span class="kn">import</span> <span class="n">Vectors</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">StorageLevel</span>
<span class="kn">import</span> <span class="nn">systemml</span> <span class="kn">as</span> <span class="nn">sml</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">rand</span> 
<span class="c"># ImageNet specific parameters</span>
<span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="s">'/home/biuser/dogs_vs_cats/train'</span>
<span class="k">def</span> <span class="nf">getLabelFeatures</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
	<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
	<span class="n">vec</span> <span class="o">=</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">sml</span><span class="o">.</span><span class="n">convertImageToNumPyArr</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)),</span> <span class="n">img_shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)[</span><span class="mi">0</span><span class="p">,:])</span>
	<span class="k">if</span> <span class="n">filename</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'cat'</span><span class="p">):</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
	<span class="k">elif</span> <span class="n">filename</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'dog'</span><span class="p">):</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
	<span class="k">else</span><span class="p">:</span>
		<span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Expected the filename to start with either cat or dog'</span><span class="p">)</span>
<span class="n">list_jpeg_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>
<span class="c"># 10 files per partition</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">list_jpeg_files</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">list_jpeg_files</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">filename</span> <span class="p">:</span> <span class="n">getLabelFeatures</span><span class="p">(</span><span class="n">filename</span><span class="p">))</span><span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s">'label'</span><span class="p">,</span> <span class="s">'features'</span><span class="p">])</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">rand</span><span class="p">())</span>
<span class="c"># Optional: but helps seperates conversion-related from training</span>
<span class="c"># Alternatively, this dataframe can be passed directly to `caffe2dml_model.fit(train_df)`</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">'kaggle-cats-dogs.parquet'</span><span class="p">)</span>
</code></pre>
</div>

<p>An alternative way to load images into a PySpark DataFrame for prediction, is to use MLLib&#8217;s LabeledPoint class:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">list_jpeg_files</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">list_jpeg_files</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">list_jpeg_files</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">filename</span> <span class="p">:</span> <span class="n">LabeledPoint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sml</span><span class="o">.</span><span class="n">convertImageToNumPyArr</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">train_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)),</span> <span class="n">img_shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)[</span><span class="mi">0</span><span class="p">,:]))</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">'features'</span><span class="p">)</span>
<span class="c"># Note: convertVectorColumnsToML has an additional serialization cost</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">MLUtils</span><span class="o">.</span><span class="n">convertVectorColumnsToML</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>
</code></pre>
</div>

<h4 id="can-i-use-caffe2dml-via-scala-">Can I use Caffe2DML via Scala ?</h4>

<p>Though we recommend using Caffe2DML via its Python interfaces, it is possible to use it by creating an object of the class
<code class="highlighter-rouge">org.apache.sysml.api.dl.Caffe2DML</code>. It is important to note that Caffe2DML&#8217;s scala API is packaged in <code class="highlighter-rouge">systemml-*-extra.jar</code>.</p>

<h4 id="how-can-i-get-summary-information-of-my-network-">How can I get summary information of my network ?</h4>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">lenet</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre>
</div>

<p>Output:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>+-----+---------------+--------------+------------+---------+-----------+---------+
| Name|           Type|        Output|      Weight|     Bias|        Top|   Bottom|
+-----+---------------+--------------+------------+---------+-----------+---------+
|mnist|           Data| (, 1, 28, 28)|            |         |mnist,mnist|         |
|conv1|    Convolution|(, 32, 28, 28)|   [32 X 25]| [32 X 1]|      conv1|    mnist|
|relu1|           ReLU|(, 32, 28, 28)|            |         |      relu1|    conv1|
|pool1|        Pooling|(, 32, 14, 14)|            |         |      pool1|    relu1|
|conv2|    Convolution|(, 64, 14, 14)|  [64 X 800]| [64 X 1]|      conv2|    pool1|
|relu2|           ReLU|(, 64, 14, 14)|            |         |      relu2|    conv2|
|pool2|        Pooling|  (, 64, 7, 7)|            |         |      pool2|    relu2|
|  ip1|   InnerProduct| (, 512, 1, 1)|[3136 X 512]|[1 X 512]|        ip1|    pool2|
|relu3|           ReLU| (, 512, 1, 1)|            |         |      relu3|      ip1|
|drop1|        Dropout| (, 512, 1, 1)|            |         |      drop1|    relu3|
|  ip2|   InnerProduct|  (, 10, 1, 1)|  [512 X 10]| [1 X 10]|        ip2|    drop1|
| loss|SoftmaxWithLoss|  (, 10, 1, 1)|            |         |       loss|ip2,mnist|
+-----+---------------+--------------+------------+---------+-----------+---------+
</code></pre>
</div>

<h4 id="how-can-i-view-the-script-generated-by-caffe2dml-">How can I view the script generated by Caffe2DML ?</h4>

<p>To view the generated DML script (and additional debugging information), please set the <code class="highlighter-rouge">debug</code> parameter to True.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">lenet</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">debug</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre>
</div>

<p>Output:</p>
<div class="highlighter-rouge"><pre class="highlight"><code>001|debug = TRUE
002|source("nn/layers/softmax.dml") as softmax
003|source("nn/layers/cross_entropy_loss.dml") as cross_entropy_loss
004|source("nn/layers/conv2d_builtin.dml") as conv2d_builtin
005|source("nn/layers/relu.dml") as relu
006|source("nn/layers/max_pool2d_builtin.dml") as max_pool2d_builtin
007|source("nn/layers/affine.dml") as affine
008|source("nn/layers/dropout.dml") as dropout
009|source("nn/optim/sgd_momentum.dml") as sgd_momentum
010|source("nn/layers/l2_reg.dml") as l2_reg
011|X_full_path = ifdef($X, " ")
012|X_full = read(X_full_path)
013|y_full_path = ifdef($y, " ")
014|y_full = read(y_full_path)
015|num_images = nrow(y_full)
016|# Convert to one-hot encoding (Assumption: 1-based labels)
017|y_full = table(seq(1,num_images,1), y_full, num_images, 10)
018|weights = ifdef($weights, " ")
019|# Initialize the layers and solvers
020|X_full = X_full * 0.00390625
021|BATCH_SIZE = 64
022|[conv1_weight,conv1_bias] = conv2d_builtin::init(32,1,5,5)
023|[conv2_weight,conv2_bias] = conv2d_builtin::init(64,32,5,5)
024|[ip1_weight,ip1_bias] = affine::init(3136,512)
025|[ip2_weight,ip2_bias] = affine::init(512,10)
026|conv1_weight_v = sgd_momentum::init(conv1_weight)
027|conv1_bias_v = sgd_momentum::init(conv1_bias)
028|conv2_weight_v = sgd_momentum::init(conv2_weight)
029|conv2_bias_v = sgd_momentum::init(conv2_bias)
030|ip1_weight_v = sgd_momentum::init(ip1_weight)
031|ip1_bias_v = sgd_momentum::init(ip1_bias)
032|ip2_weight_v = sgd_momentum::init(ip2_weight)
033|ip2_bias_v = sgd_momentum::init(ip2_bias)
034|num_validation = 10 * BATCH_SIZE
035|# Sanity check to ensure that validation set is not too large
036|if(num_validation &gt; ceil(0.3 * num_images)) {
037|    max_test_iter = floor(ceil(0.3 * num_images) / BATCH_SIZE)
038|    stop("Too large validation size. Please reduce test_iter to " + max_test_iter)
039|}
040|X = X_full[(num_validation+1):num_images,]; y = y_full[(num_validation+1):num_images,]; X_val = X_full[1:num_validation,]; y_val = y_full[1:num_validation,]; num_images = nrow(y)
041|num_iters_per_epoch = ceil(num_images / BATCH_SIZE)
042|max_epochs = ceil(2000 / num_iters_per_epoch)
043|iter = 0
044|lr = 0.01
045|for(e in 1:max_epochs) {
046|    for(i in 1:num_iters_per_epoch) {
047|            beg = ((i-1) * BATCH_SIZE) %% num_images + 1; end = min(beg + BATCH_SIZE - 1, num_images); Xb = X[beg:end,]; yb = y[beg:end,];
048|            iter = iter + 1
049|            # Perform forward pass
050|            [out3,ignoreHout_3,ignoreWout_3] = conv2d_builtin::forward(Xb,conv1_weight,conv1_bias,1,28,28,5,5,1,1,2,2)
051|            out4 = relu::forward(out3)
052|            [out5,ignoreHout_5,ignoreWout_5] = max_pool2d_builtin::forward(out4,32,28,28,2,2,2,2,0,0)
053|            [out6,ignoreHout_6,ignoreWout_6] = conv2d_builtin::forward(out5,conv2_weight,conv2_bias,32,14,14,5,5,1,1,2,2)
054|            out7 = relu::forward(out6)
055|            [out8,ignoreHout_8,ignoreWout_8] = max_pool2d_builtin::forward(out7,64,14,14,2,2,2,2,0,0)
056|            out9 = affine::forward(out8,ip1_weight,ip1_bias)
057|            out10 = relu::forward(out9)
058|            [out11,mask11] = dropout::forward(out10,0.5,-1)
059|            out12 = affine::forward(out11,ip2_weight,ip2_bias)
060|            out13 = softmax::forward(out12)
061|            # Perform backward pass
062|            dProbs = cross_entropy_loss::backward(out13,yb); dOut13 = softmax::backward(dProbs,out12); dOut13_12 = dOut13; dOut13_2 = dOut13;
063|            [dOut12,ip2_dWeight,ip2_dBias] = affine::backward(dOut13_12,out11,ip2_weight,ip2_bias); dOut12_11 = dOut12;
064|            dOut11 = dropout::backward(dOut12_11,out10,0.5,mask11); dOut11_10 = dOut11;
065|            dOut10 = relu::backward(dOut11_10,out9); dOut10_9 = dOut10;
066|            [dOut9,ip1_dWeight,ip1_dBias] = affine::backward(dOut10_9,out8,ip1_weight,ip1_bias); dOut9_8 = dOut9;
067|            dOut8 = max_pool2d_builtin::backward(dOut9_8,7,7,out7,64,14,14,2,2,2,2,0,0); dOut8_7 = dOut8;
068|            dOut7 = relu::backward(dOut8_7,out6); dOut7_6 = dOut7;
069|            [dOut6,conv2_dWeight,conv2_dBias] = conv2d_builtin::backward(dOut7_6,14,14,out5,conv2_weight,conv2_bias,32,14,14,5,5,1,1,2,2); dOut6_5 = dOut6;
070|            dOut5 = max_pool2d_builtin::backward(dOut6_5,14,14,out4,32,28,28,2,2,2,2,0,0); dOut5_4 = dOut5;
071|            dOut4 = relu::backward(dOut5_4,out3); dOut4_3 = dOut4;
072|            [dOut3,conv1_dWeight,conv1_dBias] = conv2d_builtin::backward(dOut4_3,28,28,Xb,conv1_weight,conv1_bias,1,28,28,5,5,1,1,2,2); dOut3_2 = dOut3;
073|            # Update the parameters
074|            conv1_dWeight_reg = l2_reg::backward(conv1_weight, 5.000000237487257E-4)
075|            conv1_dWeight = conv1_dWeight + conv1_dWeight_reg
076|            [conv1_weight,conv1_weight_v] = sgd_momentum::update(conv1_weight,conv1_dWeight,(lr * 1.0),0.8999999761581421,conv1_weight_v)
077|            [conv1_bias,conv1_bias_v] = sgd_momentum::update(conv1_bias,conv1_dBias,(lr * 2.0),0.8999999761581421,conv1_bias_v)
078|            conv2_dWeight_reg = l2_reg::backward(conv2_weight, 5.000000237487257E-4)
079|            conv2_dWeight = conv2_dWeight + conv2_dWeight_reg
080|            [conv2_weight,conv2_weight_v] = sgd_momentum::update(conv2_weight,conv2_dWeight,(lr * 1.0),0.8999999761581421,conv2_weight_v)
081|            [conv2_bias,conv2_bias_v] = sgd_momentum::update(conv2_bias,conv2_dBias,(lr * 2.0),0.8999999761581421,conv2_bias_v)
082|            ip1_dWeight_reg = l2_reg::backward(ip1_weight, 5.000000237487257E-4)
083|            ip1_dWeight = ip1_dWeight + ip1_dWeight_reg
084|            [ip1_weight,ip1_weight_v] = sgd_momentum::update(ip1_weight,ip1_dWeight,(lr * 1.0),0.8999999761581421,ip1_weight_v)
085|            [ip1_bias,ip1_bias_v] = sgd_momentum::update(ip1_bias,ip1_dBias,(lr * 2.0),0.8999999761581421,ip1_bias_v)
086|            ip2_dWeight_reg = l2_reg::backward(ip2_weight, 5.000000237487257E-4)
087|            ip2_dWeight = ip2_dWeight + ip2_dWeight_reg
088|            [ip2_weight,ip2_weight_v] = sgd_momentum::update(ip2_weight,ip2_dWeight,(lr * 1.0),0.8999999761581421,ip2_weight_v)
089|            [ip2_bias,ip2_bias_v] = sgd_momentum::update(ip2_bias,ip2_dBias,(lr * 2.0),0.8999999761581421,ip2_bias_v)
090|            # Compute training loss &amp; accuracy
091|            if(iter  %% 100 == 0) {
092|                    loss = 0
093|                    accuracy = 0
094|                    tmp_loss = cross_entropy_loss::forward(out13,yb)
095|                    loss = loss + tmp_loss
096|                    true_yb = rowIndexMax(yb)
097|                    predicted_yb = rowIndexMax(out13)
098|                    accuracy = mean(predicted_yb == true_yb)*100
099|                    training_loss = loss
100|                    training_accuracy = accuracy
101|                    print("Iter:" + iter + ", training loss:" + training_loss + ", training accuracy:" + training_accuracy)
102|                    if(debug) {
103|                            num_rows_error_measures = min(10, ncol(yb))
104|                            error_measures = matrix(0, rows=num_rows_error_measures, cols=5)
105|                            for(class_i in 1:num_rows_error_measures) {
106|                                    tp = sum( (true_yb == predicted_yb) * (true_yb == class_i) )
107|                                    tp_plus_fp = sum( (predicted_yb == class_i) )
108|                                    tp_plus_fn = sum( (true_yb == class_i) )
109|                                    precision = tp / tp_plus_fp
110|                                    recall = tp / tp_plus_fn
111|                                    f1Score = 2*precision*recall / (precision+recall)
112|                                    error_measures[class_i,1] = class_i
113|                                    error_measures[class_i,2] = precision
114|                                    error_measures[class_i,3] = recall
115|                                    error_measures[class_i,4] = f1Score
116|                                    error_measures[class_i,5] = tp_plus_fn
117|                            }
118|                            print("class    \tprecision\trecall  \tf1-score\tnum_true_labels\n" + toString(error_measures, decimal=7, sep="\t"))
119|                    }
120|            }
121|            # Compute validation loss &amp; accuracy
122|            if(iter  %% 500 == 0) {
123|                    loss = 0
124|                    accuracy = 0
125|                    validation_loss = 0
126|                    validation_accuracy = 0
127|                    for(iVal in 1:num_iters_per_epoch) {
128|                            beg = ((iVal-1) * BATCH_SIZE) %% num_validation + 1; end = min(beg + BATCH_SIZE - 1, num_validation); Xb = X_val[beg:end,]; yb = y_val[beg:end,];
129|                            # Perform forward pass
130|                            [out3,ignoreHout_3,ignoreWout_3] = conv2d_builtin::forward(Xb,conv1_weight,conv1_bias,1,28,28,5,5,1,1,2,2)
131|                            out4 = relu::forward(out3)
132|                            [out5,ignoreHout_5,ignoreWout_5] = max_pool2d_builtin::forward(out4,32,28,28,2,2,2,2,0,0)
133|                            [out6,ignoreHout_6,ignoreWout_6] = conv2d_builtin::forward(out5,conv2_weight,conv2_bias,32,14,14,5,5,1,1,2,2)
134|                            out7 = relu::forward(out6)
135|                            [out8,ignoreHout_8,ignoreWout_8] = max_pool2d_builtin::forward(out7,64,14,14,2,2,2,2,0,0)
136|                            out9 = affine::forward(out8,ip1_weight,ip1_bias)
137|                            out10 = relu::forward(out9)
138|                            [out11,mask11] = dropout::forward(out10,0.5,-1)
139|                            out12 = affine::forward(out11,ip2_weight,ip2_bias)
140|                            out13 = softmax::forward(out12)
141|                            tmp_loss = cross_entropy_loss::forward(out13,yb)
142|                            loss = loss + tmp_loss
143|                            true_yb = rowIndexMax(yb)
144|                            predicted_yb = rowIndexMax(out13)
145|                            accuracy = mean(predicted_yb == true_yb)*100
146|                            validation_loss = validation_loss + loss
147|                            validation_accuracy = validation_accuracy + accuracy
148|                    }
149|                    validation_accuracy = validation_accuracy / num_iters_per_epoch
150|                    print("Iter:" + iter + ", validation loss:" + validation_loss + ", validation accuracy:" + validation_accuracy)
151|            }
152|    }
153|    # Learning rate
154|    lr = (0.009999999776482582 * 0.949999988079071^e)
155|}

Iter:100, training loss:0.24014199350958168, training accuracy:87.5
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       3.0000000
2.0000000       1.0000000       1.0000000       1.0000000       8.0000000
3.0000000       0.8888889       0.8888889       0.8888889       9.0000000
4.0000000       0.7500000       0.7500000       0.7500000       4.0000000
5.0000000       0.7500000       1.0000000       0.8571429       3.0000000
6.0000000       0.8333333       1.0000000       0.9090909       5.0000000
7.0000000       1.0000000       1.0000000       1.0000000       8.0000000
8.0000000       0.8571429       0.7500000       0.8000000       8.0000000
9.0000000       1.0000000       0.5714286       0.7272727       7.0000000
10.0000000      0.7272727       0.8888889       0.8000000       9.0000000

Iter:200, training loss:0.09555593867171894, training accuracy:98.4375
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       10.0000000
2.0000000       1.0000000       1.0000000       1.0000000       3.0000000
3.0000000       1.0000000       1.0000000       1.0000000       9.0000000
4.0000000       1.0000000       1.0000000       1.0000000       6.0000000
5.0000000       1.0000000       1.0000000       1.0000000       7.0000000
6.0000000       1.0000000       1.0000000       1.0000000       8.0000000
7.0000000       1.0000000       0.6666667       0.8000000       3.0000000
8.0000000       1.0000000       1.0000000       1.0000000       9.0000000
9.0000000       0.8571429       1.0000000       0.9230769       6.0000000
10.0000000      1.0000000       1.0000000       1.0000000       3.0000000

Iter:300, training loss:0.058686794512570216, training accuracy:98.4375
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       6.0000000
2.0000000       1.0000000       1.0000000       1.0000000       9.0000000
3.0000000       1.0000000       1.0000000       1.0000000       4.0000000
4.0000000       1.0000000       1.0000000       1.0000000       8.0000000
5.0000000       1.0000000       1.0000000       1.0000000       6.0000000
6.0000000       1.0000000       0.8750000       0.9333333       8.0000000
7.0000000       1.0000000       1.0000000       1.0000000       5.0000000
8.0000000       1.0000000       1.0000000       1.0000000       2.0000000
9.0000000       0.8888889       1.0000000       0.9411765       8.0000000
10.0000000      1.0000000       1.0000000       1.0000000       8.0000000

Iter:400, training loss:0.08742103541529415, training accuracy:96.875
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       6.0000000
2.0000000       0.8000000       1.0000000       0.8888889       8.0000000
3.0000000       1.0000000       0.8333333       0.9090909       6.0000000
4.0000000       1.0000000       1.0000000       1.0000000       4.0000000
5.0000000       1.0000000       1.0000000       1.0000000       4.0000000
6.0000000       1.0000000       1.0000000       1.0000000       6.0000000
7.0000000       1.0000000       1.0000000       1.0000000       7.0000000
8.0000000       1.0000000       1.0000000       1.0000000       6.0000000
9.0000000       1.0000000       1.0000000       1.0000000       4.0000000
10.0000000      1.0000000       0.9230769       0.9600000       13.0000000

Iter:500, training loss:0.05873836245880005, training accuracy:98.4375
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       3.0000000
2.0000000       1.0000000       1.0000000       1.0000000       5.0000000
3.0000000       1.0000000       1.0000000       1.0000000       6.0000000
4.0000000       1.0000000       1.0000000       1.0000000       9.0000000
5.0000000       1.0000000       1.0000000       1.0000000       4.0000000
6.0000000       1.0000000       0.8571429       0.9230769       7.0000000
7.0000000       0.8571429       1.0000000       0.9230769       6.0000000
8.0000000       1.0000000       1.0000000       1.0000000       9.0000000
9.0000000       1.0000000       1.0000000       1.0000000       10.0000000
10.0000000      1.0000000       1.0000000       1.0000000       5.0000000

Iter:500, validation loss:260.1580978627665, validation accuracy:96.43954918032787
Iter:600, training loss:0.07584116043829209, training accuracy:98.4375
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       8.0000000
2.0000000       1.0000000       1.0000000       1.0000000       4.0000000
3.0000000       1.0000000       1.0000000       1.0000000       4.0000000
4.0000000       1.0000000       1.0000000       1.0000000       4.0000000
5.0000000       1.0000000       1.0000000       1.0000000       5.0000000
6.0000000       1.0000000       1.0000000       1.0000000       8.0000000
7.0000000       1.0000000       1.0000000       1.0000000       8.0000000
8.0000000       1.0000000       0.9230769       0.9600000       13.0000000
9.0000000       1.0000000       1.0000000       1.0000000       5.0000000
10.0000000      0.8333333       1.0000000       0.9090909       5.0000000

Iter:700, training loss:0.07973166944626336, training accuracy:98.4375
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       5.0000000
2.0000000       1.0000000       1.0000000       1.0000000       4.0000000
3.0000000       1.0000000       1.0000000       1.0000000       6.0000000
4.0000000       1.0000000       1.0000000       1.0000000       4.0000000
5.0000000       1.0000000       1.0000000       1.0000000       5.0000000
6.0000000       1.0000000       1.0000000       1.0000000       6.0000000
7.0000000       1.0000000       1.0000000       1.0000000       10.0000000
8.0000000       0.8000000       1.0000000       0.8888889       4.0000000
9.0000000       1.0000000       1.0000000       1.0000000       8.0000000
10.0000000      1.0000000       0.9166667       0.9565217       12.0000000

Iter:800, training loss:0.0063778595034221855, training accuracy:100.0
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       9.0000000
2.0000000       1.0000000       1.0000000       1.0000000       6.0000000
3.0000000       1.0000000       1.0000000       1.0000000       7.0000000
4.0000000       1.0000000       1.0000000       1.0000000       7.0000000
5.0000000       1.0000000       1.0000000       1.0000000       4.0000000
6.0000000       1.0000000       1.0000000       1.0000000       9.0000000
7.0000000       1.0000000       1.0000000       1.0000000       6.0000000
8.0000000       1.0000000       1.0000000       1.0000000       8.0000000
9.0000000       1.0000000       1.0000000       1.0000000       2.0000000
10.0000000      1.0000000       1.0000000       1.0000000       6.0000000

Iter:900, training loss:0.019673112167879484, training accuracy:100.0
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       3.0000000
2.0000000       1.0000000       1.0000000       1.0000000       4.0000000
3.0000000       1.0000000       1.0000000       1.0000000       3.0000000
4.0000000       1.0000000       1.0000000       1.0000000       5.0000000
5.0000000       1.0000000       1.0000000       1.0000000       6.0000000
6.0000000       1.0000000       1.0000000       1.0000000       10.0000000
7.0000000       1.0000000       1.0000000       1.0000000       7.0000000
8.0000000       1.0000000       1.0000000       1.0000000       7.0000000
9.0000000       1.0000000       1.0000000       1.0000000       12.0000000
10.0000000      1.0000000       1.0000000       1.0000000       7.0000000

Iter:1000, training loss:0.06137978002508307, training accuracy:96.875
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       5.0000000
2.0000000       1.0000000       1.0000000       1.0000000       7.0000000
3.0000000       1.0000000       1.0000000       1.0000000       8.0000000
4.0000000       0.8333333       0.8333333       0.8333333       6.0000000
5.0000000       1.0000000       1.0000000       1.0000000       5.0000000
6.0000000       1.0000000       1.0000000       1.0000000       10.0000000
7.0000000       1.0000000       1.0000000       1.0000000       3.0000000
8.0000000       0.8888889       0.8888889       0.8888889       9.0000000
9.0000000       1.0000000       1.0000000       1.0000000       7.0000000
10.0000000      1.0000000       1.0000000       1.0000000       4.0000000

Iter:1000, validation loss:238.62301345198944, validation accuracy:97.02868852459017
Iter:1100, training loss:0.023325103696013115, training accuracy:100.0
class           precision       recall          f1-score        num_true_labels
1.0000000       1.0000000       1.0000000       1.0000000       4.0000000
2.0000000       1.0000000       1.0000000       1.0000000       10.0000000
3.0000000       1.0000000       1.0000000       1.0000000       6.0000000
4.0000000       1.0000000       1.0000000       1.0000000       4.0000000
5.0000000       1.0000000       1.0000000       1.0000000       2.0000000
6.0000000       1.0000000       1.0000000       1.0000000       10.0000000
7.0000000       1.0000000       1.0000000       1.0000000       7.0000000
8.0000000       1.0000000       1.0000000       1.0000000       6.0000000
9.0000000       1.0000000       1.0000000       1.0000000       9.0000000
10.0000000      1.0000000       1.0000000       1.0000000       6.0000000
...
</code></pre>
</div>


        </div> <!-- /container -->

        

        <script src="js/vendor/jquery-1.12.0.min.js"></script>
        <script src="js/vendor/bootstrap.min.js"></script>
        <script src="js/vendor/anchor.min.js"></script>
        <script src="js/main.js"></script>
        





        <!-- MathJax Section -->
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script>
            // Note that we load MathJax this way to work with local file (file://), HTTP and HTTPS.
            // We could use "//cdn.mathjax...", but that won't support "file://".
            (function(d, script) {
                script = d.createElement('script');
                script.type = 'text/javascript';
                script.async = true;
                script.onload = function(){
                    MathJax.Hub.Config({
                        tex2jax: {
                            inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ],
                            displayMath: [ ["$$","$$"], ["\\[", "\\]"] ],
                            processEscapes: true,
                            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
                        }
                    });
                };
                script.src = ('https:' == document.location.protocol ? 'https://' : 'http://') +
                    'cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
                d.getElementsByTagName('head')[0].appendChild(script);
            }(document));
        </script>
    </body>
</html>
